So let's review the story so far.
We've been discussing the QuickSort algorithm.
Here again is the high level description.
So QuickSort you call two subroutines first and
then you make two recursive calls.
So the first subroutine, Choose Pivot, we haven't discussed yet at all.
That'll be one of the main topics of this video.
But the job of the Choose Pivot subroutine is to somehow select one of the n
elements in the input array to act as a pivot element.
Then what does it mean to be a pivot?
Well that comes into play in the second subroutine.
The partition subroutine, which we did discuss quite a bit in a previous video.
So what partition does is it rearranges the elements in the input array, so
that it has the following property.
So that the pivot p winds up in its rightful position.
That is, it's to the right of all of the elements less than it, and
it's to the left of all of the elements bigger than it.
The stuff less than it's to the left in some jumbled order,
the stuff bigger than it's to the right in some jumbled order.
That's what's, is listed here as the first part and
the second part of the partitioned array.
Now once you've done this partitioning, you're good to go.
You just recursively solve you recursively sort the first part,
[INAUDIBLE] the right order,
you call QuickSort again to recursively sort the right part, and bingo.
The entire array is sorted.
You don't need a combine step.
You don't need a merge step.
Moreover recall in a previous video we saw that the partition array can be
implemented in linear time, and
moreover it works in place with essentially no additional storage.
We also in an optional video formally prove the correctness of QuickSort.
And remember QuickSort is independent of how you implement
the Choose Pivot subroutine.
So what we're going to do now, is discuss the running time of
the QuickSort algorithm, and this is where the choice of the pivot is very important.
So what everybody should be wondering about at this point, is,
is QuickSort a good algorithm?
Does it run fast?
The bar's pretty high,
we already have MergeSort which is a very excellent practical and logging algorithm.
The key point to realize at this juncture is that we are not currently in
a position to discuss the running time of the QuickSort algorithm.
The reason is we do not have enough information.
The running time of QuickSort depends crucially on how you choose the pivot.
It depends crucially on the quality of the pivot chosen.
You'd be right to wonder what I mean by a pivot's quality.
And basically what I mean is a pivot is good if it splits the partitioned array
into roughly two equal sized sub-problems.
And it's bad, it's of low quality if we get very unbalanced sub-problems.
So to understand both what I mean in the ramifications of having good quality and
bad quality pivots, let's walk through a couple of quiz questions.
This first quiz question is meant to explore a sort of worse case execution of
the QuickSort algorithm.
What happens when you choose pivots that are very poorly suited for
the particular input array?
Maybe more specific, suppose we use the most naive choose pivot implementation,
like we're discussing in the partition video.
So remember here we just pluck out the first element of the array, and
we use that as the pivot.
So suppose that's how we implement the ChoosePivot sub-routine and
more over, suppose that the input array to QuickSort is an array
that's already in sorted order.
So for example, if it just had the numbers one through eight, it would be one,
two, three, four, five, six, seven, eight in order.
My question for you is, what is the running time of
this recursive QuickSort algorithm on an already sorted array,
if we always use the first element of a sub-array as the pivot?
Okay, so this is slightly tricky but actually a very important question.
So the answer is the fourth one.
So it turns out that QuickSort, if you pass it an already sorted array and
you're using the first element as pivot elements, it runs in quadratic time.
And remember, for a sorting algorithm, quadratic is bad.
It's bad in the sense that we can do better, MergeSort runs in time n log n,
which is much better than n squared.
And if we are happy with an n squared running time,
we wouldn't have to resort to these sort of relatively exotic sorting algorithms.
We could just use insertion sort and we'd be fine.
We'd get that same quadratic running time.
Okay, so now I owe you an explanation.
Why is it that QuickSort can actually run in quadratic time,
in this unlucky case of being passed already sorted input array.
Well to understand, let's think about what pivot gets chosen and
what are the ramifications of that pivot choice for
how the array gets partitioned, and then what the recursion looks like.
So, lets just think of the array as being the numbers one through n in sorted order.
What is going to be our pivot?
Well by definition, we're choosing the first element of the Pivot, so
the Pivots are just going to be one.
Now we're going to invoke the partition subroutine, and
if you go back to the pseudo code of the partition sub routine, you'll notice that
if we pass an already sorted array, it's going to do essentially nothing.
because it's just going to advance the index in j until it falls off at
the end of the array, and
it's just going to return back to us this same array that it was pathed as input.
So partition sub routine if given an already sorted array,
returns an already sorted array.
Okay? So we have just the pivot one in
the first position, and
then the numbers two through n in order in the remainder of the positions.
So if we draw our usual picture of what a partitioned array looks like,
with everything less than the pivot to the left,
everything bigger than the pivot to the right.
Well, since nothing is less than the pivot, this stuff is going to be empty.
This will not exist.
And to the right of the pivot, this will have length n minus 1, and
moreover it will still be sorted.
So once partition completes,
we go back to the outer call of QuickSort which then calls itself recursively twice.
Now in this case one of the recursive calls is just vacuous.
There's just and empty array, there's nothing to do.
So really there's only one recursive call, and
that happens on a problem of size only one less.
So this is about the most unbalanced split we could possibly see.
All right, well one side has 0 elements, one size n minus 1.
So don't really get any worse than that.
And this is going to keep happening over, and over, and over again.
We're going to recurse on the numbers two through n,
we're going to choose the first element, the two as the pivot.
Again we'll feed it to partition, we'll get back the exact same subarray that
we handed it in, we'll get through the numbers 2 through n in sorted order.
We exclude the pivot two, recurse on the numbers three through n.
A subarray of length n minus two.
The next recursion level we recurse on an array of size of length n minus 3,
then n minus 4, then n minus 5 and so on, until finally, after,
at a recursion depth of n roughly, we would get down to just the last element n.
The base case kicks in, and we return that and QuickSort completes.
So that's how QuickSort is going to execute on this particular input with
these particular pivot choices.
So what running time does that give to us?
Well, the first observation is that, you know, in each recursive call,
we do have to invoke the partition subroutine.
And the partition subroutine does look at every element in the array it
is passed as input.
So if we pass partition in array of linked K, it's going to do atleast K
operations because it looks at each element at least once.
So the runtime is going to be bounded below,
by the work we do in the outermost call, which is on an array of
linked n plus the amount we do in the second, a level of recursion, which is on
a sub-array of linked minus 1 plus n minus 2, plus blah-blah-blah-blah-blah.
All the way down to plus 1 for the very last level of recursion.
So this is a lower band on our running time, and
this is already theta of n squared.
So one easy way to see why this sum n plus n minus 1 plus et cetera, et cetera,
leads to a bound of n squared is to just focus on the first half of the terms.
So the first n over two terms in the sum are all of magnitude at least n over 2.
So the sum is at least N squared over 4.
It's also evident that this sum is at most n squared.
So overall,
the running time of Quicksort on this bad input is going to be quadratic.
Now having understood what the worst case performance of the Quicksort algorithm is,
let's move on to discuss it's best case running time.
Now we don't generally care about the best case performance of algorithms for
it's own sake.
The reason that we want to think about Quicksort in the best case.
First of all, it'll give us better intuition for how the algorithm works.
Second of all, it'll draw a line in the sand.
Its average case running time certainly can't be better than the best case, so
this will give us a target for what we're shooting for
in our subsequent mathematical analysis.
So what was the best case?
What is the highest quality pivot we could hope for?
Well again, we think of the quality of a pivot as the amount of balance that it
provides between the two subproblems.
So ideally we chose a pivot which gave us two sub-problems both of size and
over two or less.
And there's a name for
the element that would give us that perfectly balanced split.
It's the median element of the array, okay?
The element where exactly half of the elements are less than it, and
half the elements are bigger than it.
That would give us essentially a perfect 50/50 split of the input array.
So here's the question.
Suppose we had some input and we ran QuickSort and
everything just worked out in our favor in the, magically in the best possible way.
That is in every single recursive implication of
Quicksort on any sub-array of the original input array,
suppose we happen to get as our pivot the median element.
That is supposing every single recursive call we wind up getting a perfect 50 50
split of the input array before we recurse.
This question asks you to analyze the running time of this algorithm in
this magical best case scenario.
So the answer to this third to this question is the third option.
The answer is it runs in and login time.
Why is that?
Well the reason is that then the recurrence which governs the running time
of quick sort is, exactly matches the recurrence that governs,
the merge sort running time, which we already know is n log n.
That is the running time QuickSort requires in this magically special case on
an array of length n, well as usually, you have a recurrence in two parts.
There's the work that gets done by the recursive calls and
then there's the work that gets done now.
Now by assumption, we wind up picking the median as the pivot.
So, there's going to be two recursive calls,
each of which will be on an input of size at most N over 2.
And we can write this, this is because the pivot equals the median.
So this is not true for Quicksort in general.
It's only true in this magical case where the pivot is the median.
So that's what gets done by the two recursive calls and
then how much work do we do outside of the recursive calls?
Well, we have to do the two pivots sub routine and
I guess strictly speaking I haven't said how that was implemented.
But let's assume that choose pivot does only a linear amount of work, and then as
we've seen the partition sub routine only does a linear amount of work as well.
So let's say O of n for work outside of the recursive calls.
And what do we know?
We know this implies, say by using the master method or
by just using the exact same argument as for MergeSort.
This gives us a running time bound of n log n.
And then again something I haven't really been emphasizing though which is true,
is that actually we can right theta of n log n.
And that's because in the recurrence, in fact we know that the work done outside of
the recursive causes exactly theta of n, okay?
Partition needs really linear time, not just bit O of n time.
In fact the work done outside the recursive calls is theta of n,
that's because the partition sub routine does indeed look at every entry in
the array that it passed.
And as a result, we didn't really discuss this so much in the master method but
as I mentioned in passing if you have recurrences which are tight in
this sense then the, the results of the master method can also be
strengthened to be theta instead of just big O.
But those are just some extra details.
The upshot of this quiz is that even in the best case,
even if we magically get perfect pivots throughout the entire trajectory of
QuickSort, the best we can hope for is a n log n upper bound.
It's not going to get any better than that.
So the question is, how do we have a principled way of choosing pivots so that
we get this best case or something like it, this best case n log n running time?
So that's what that problem that we have to solve next.
So the last couple of quizzes have identified super important question as
far as the implementation of QuickSort,
which is how are we going to choose these pivots?
Right? We now know that they have a big
influence on the running time of our algorithm.
It could be bad as n squared or as good as n log n.
And we really want to be on the n log n side.
So the key question, how to choose pivots.
And Quicksort is the first killer application we're going to see of the idea
of randomized algorithms, that is allowing your algorithms to flip coins in the code,
so that you get some kind of good performance on average.
So the big idea, is random pivots.
By which I mean for every time we recursively call QuickSort and we, or pass
some subarray of length K, among the K candidate pivot elements in the sub array,
we're going to choose each one with eq, equally likely with probability 1 over K.
And we're going to make a new random choice every time we
have a recursive call, and then we're just going to see how the algorithm does.
So this is our first example of a randomized algorithm.
This is an algorithm where if you feed it exactly the same input,
it will actually run differently on different executions.
And that's because there's randomness internal to the code of the algorithm.
Now it's not necessarily intuitive that randomization should have any
purpose in computation, in software design, and algorithm design.
But in fact, and this has been sort of one of the real breakthroughs in
algorithm design, mostly in the 70s, realizing how important this is.
That the use of randomization can make algorithms more elegant, simpler,
easier to code, more faster, or just simply you can solve problems that you
could not solve or at least not solve as easily without the use of randomization.
So it's really one thing that should be in your toolbox as
an algorithm designer, randomization.
QuickSort will be the first cal, killer application but
we'll see a couple more later in the course.
Now by the end of this sequence of videos,
I'll have given you a complete rigorous argument about why this works.
Why with random pivots, QuickSort always runs very quickly on average.
But, you know, before moving into sa,
into anything too formal, let's develop a little bit of intuition or
at least kind of a day dream about why on earth could this could possibly work?
Why on earth this could possibly be a good idea to have randomness internal to
our QuickSort implementation.
Well, so first, just you know, very high level, what would be sort of the hope or
the dream?
The hope would be you know, random pivots, they're not going to be perfect.
I mean, you're not going to just sort of guess the median.
You only have a one in n chance of figuring out which one the median is.
But, the hope is that most choices of a pivot will be good enough.
So that's pretty fuzzy.
Let's drill down a little bit, and develop this intuition further.
Let me describe it in two steps.
The first claim is that, you know, in our last quiz we said suppose we get lucky and
we always pick the median in every single recursive call.
And we observe that we do great, we get n log n running time.
So now let's observe that actually to get the n log n running time,
it's not important that we magically get the median every single recursive call.
If we get any kind of reasonable pivot, by which a pivot that gives us some kind of
approximately balanced split of the problems.
Again, we're going to be good.
Okay? So the last quiz
wasn't really particular to getting the exact median.
Near medians are also fine.
To be concrete, suppose we always pick a pivot which guarantees us a split of 25,
75 or better.
That is, both recursive calls should be called on a raise of size at most 75% of
the one that we started with.
So precisely, if we always get a 25, 75 split or better in every recursive call, I
claim that the running time of quick sort in that event will be big O of n log n.
Just like it was in the last quiz where we were actually assuming something much
stronger that we're getting the median.
Now this is not so obvious.
The fact that 25, 75 splits guaranteed n log n running time.
For those of you that are feeling keen, you might want to try to prove this.
So you can prove this using a recursion tree argument,
that because you don't have balanced sub-problems you have to
work a little bit harder than you do in the cases covered by the master method.
So that's the first part of the intuition.
And this is what we mean by a pivot being good enough.
If we get a 25, 75 split or better, we're good to go.
We get our desired, our target n log n running time.
So the second part of the intuition is to realize that actually,
we don't have to get all that lucky to just be getting a 25, 75 split.
That's actually a pretty modest goal.
And even this modest goal's enough to get the n log n running time.
Right? So suppose our array contains the numbers,
the integers between one and 100.
So it's an array of length 100.
Think for a second,
which of those elements is going to give us a split that's 25, 75 or better?
So, if we pick any element between 26 and 75, inclusive will be totally good.
Right? If we pick something that's at least 26,
that means the less sub problem is going to have at least the element 1 through 25,
that have at least 25% of the elements.
If we pick something less than 75, then the right sub problem will have all of
the elements 76 through 100 after we partition.
So that will also have at least 25% of the elements.
So anything between 26 and 75 gives us a 75, 25 split or better.
But that's a full half of the elements.
So it's as good as just flipping a fair coin and hoping we get heads.
So with 50% probability, we get a split good enough to get the,
good enough to get this n log n bound.
And so again, the high level hope is that often enough, you know,
half of the time, we get these you know, good enough splits.
25, 75 splits or better.
So that would seem to suggest an n log n running time on average is a,
a legitimate hope.
So that's the high level intuition.
But if I were you I would certainly not be content with this somewhat hand wavy
explanation that I've given you so far.
What I've told you is sort of the hope.
The dream.
Why there is at least a chance this might work.
But, but the question remains, and
I would encourage such skepticism, which is, does this really work?
And to answer that, we're going to have to do some actual mathematical analysis.
And that's what I'm going to show you.
I'm going to show you a complete,
rigorous analysis of the quick sort algorithm with random pivots and
we'll show that, we'll show that yes, in fact it does really work.
And this highlights what's going to be a recurring theme in this course and
a recurring theme just in the study and
understanding of algorithms, which is its quite often there's some
fundamental problem when you're trying to code a solution, and
you come up with a novel idea, it might be brilliant, and it might suck.
And you have no idea.
Now obviously you can code up the idea, run it on some concrete instances and
get a feel for you know, whether it seems like a good idea or not.
But if you really want to know fundamentally what makes the idea good or
what makes the idea bad, really you need to turn to mathematical analysis to
give you a complete explanation.
And that's exactly what we're going to do with QuickSort, and
it will explain in a very deep way why it works so well.
Specifically in the next sequence of three videos,
I'm going to show you an analysis, a proof of the following theorem about QuickSort.
So under no assumptions about the data, that is for
every input array of a given length say n.
The average running time of Quicksort,
implemented with random pivots, is big O of n log n.
And again in fact it's theta of n log n,
but we'll just focus on the big O of n log n part.
So this is a very, very cool theorem about this randomized QuickSort algorithm.
One thing I want to be clear, so
that you don't undersell this guarantee in your own mind.
This is a worse case guarantee with respect to the input.
Okay, so notice at the beginning of this theorem what do we say?
For every input array of length n, all right?
So we have absolutely no assumptions about the data.
This is a totally general purpose sorting sub-routine which you can use whenever you
want, even if you have no idea where the data's coming from.
And these guarantees are still going to be true.
This of course is something I held forth about at some length back in
our guiding principles video.
When I argued that if you can get away with it, what you really want is
general purpose algorithms, which make no data assumptions so
they can be used over and over again in all kinds of different contexts.
And that still have great guarantees.
And QuickSort is one of those.
So basically if you have a dataset and it fits in the main memory of your machine,
again sorting is a for-free sub-routine, in particular, QuickSort.
The QuickSort of limitation is for free.
So this just runs so blazingly fast.
It doesn't matter what the array is,
maybe you don't even know why you want to sort it, but go ahead, why not?
Maybe it'll make your life easier like it did for example in the closest pair
algorithm for those of you that watched those two optional videos.
Now the word average does appear in this theorem.
And, you know, as I've been harping on,
this average is not over any assumptions on the data.
We're certainly not assuming that the input array is random in any sense.
The input array can be anything.
So where is the averaging coming from?
The averaging is coming only from randomness,
which is internal to our algorithm.
Randomness that we put in the code ourselves.
That we're responsible for.
So remember, randomized algorithms have the interesting property that even if
you run it on the same input over and
over again, you're going to get different executions.
So the running time of a randomized algorithm depend you know,
can vary as you run it on the same input over and over again.
The quizzes have taught us that the running time of QuickSort on
a given input, fluctuates from anywhere between the best case of
n log n to the worst case of n squared.
So what this theorem is telling us, is that for every possible input array, while
the running time does indeed fluctuate between an upper bound of n squared and
the lower bound of n log n, the best case is dominating.
On average it's n log n.
On average it's almost as good as the best case.
That's what's so amazing about QuickSort.
That was, in a square that can pop up once in a while has, doesn't matter.
You're never going to see it.
You're always going to see this n log n like behavior in randomized QuickSort.
So for
some of you I'll see you next in a video on probability review, that's optional.
For the rest of you, I'll see you in the analysis of this theorem.